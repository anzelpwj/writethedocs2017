# The Science(?) of Documentation

WriteTheDocs, Portland, 2017

Paul Anzel, Data Scientist, Metromile Inc.

## Script

### Slide 1

Hello everyone, my name is Paul Anzel, and I’m NOT a documentarian. Instead, I’m a data scientist, though one who has had to use enough ill-documented code to want to try and do better. Hence why I’m here to hang out with y’all today.

### Slide 2

Like most data-scientists, I went through the academic pipeline until I realized that was a mug’s game, but old habits die hard and I thought a good way to begin would be to see what sort of research there already is. So, I went to Google Scholar, entered in “software documentation” and looked to see what I would get.

### Slide 3

First off the bat, what I found were surveys with people complaining about the state of documentation: things are obsolete and incomplete, and programmers are going right to the source code because they don’t even know if they can trust what docs they get.

### Slide 4

So, yeah...

### Slide 5

But wait, it gets worse!

There’s not really that much more peer-reviewed research beyond that! We all know things aren’t where we want them to be, but what changes should we make to improve matters? If all I get is a Friday afternoon to document our API, should I write a tutorial or try and just do a general reference?

For the most part, people have written books and blog-posts about what’s worked for them, which in science we call a “case study”. And case studies are important! We see them all the time in medical journals. But I’d love to see us go farther.

### Slide 6

Now, I realize that this is pretty much what I sound like right now...

### Slide 7

...and this is not my intention! And, in all fairness, it’s not like my corner of the office does much better.

### Slide 8

One of my favorite authors in the tech industry is a fellow named Greg Wilson. One trenchant thing he’s said is that the field probably isn’t ready to be called “Computer Science”, but rather “Computer Two Beers and Here’s My Opinion”. There is some research, much of which he collected in this book, but it hasn’t filtered down.

Here’s one recent example--a professor named Andreas Stefik has been doing research into how beginners learn programming languages. As a control, his lab designed Randomo, a language where the syntax was randomly generated. He then had subjects try and learn a number of different languages. Turns out, the C-family languages--Java, C, and Perl--were all as hard to learn as Randomo.

The programming community--trained, experienced professionals--took this information in with measured consideration...

### Slide 9

[pause]

### Slide 10

But, my main point here is not to scold, but to say that I think we have some real opportunities here! Let’s methodically test our assumptions! If you’ve got access to a number of coders willing to try some things, let’s run some experiments! And then share your results with the rest of us!

### Slide 11

Now, there is a fair amount of educational psychology literature you can draw on, though I haven’t seen it filter into the books or blog-posts I’ve seen. This book by Ambrose is a great overview, and cites hundreds of papers to look through.

### Slide 12

And social science is difficult! Humans are unpredictable and we’re often looking for subtle effects. But here are some good rules of thumb.

First, be skeptical if people are making grand pronouncements based on only a few dozen subjects. These results often don’t hold up.

Second, if no-one shows you how uncertain they are, be careful. There’s going to be a lot of noise in the data.

Third, look to see how strong the effect is. “Statistically significant” and practically significant are two very different things.

Finally, there are different types of studies, and I have a hierarchy of trust for them. Case studies are a start, but I’d like to see us doing more surveys and randomized controlled trials! And, once we get those, we can start doing meta-analyses, where we combine these studies together.

### Slide 13

One final thing, from the world of science popularization. In the 60’s writers of the New York Times went on strike and while they continued to write articles, they did not publish anything. This ended up being a nice natural control-study on whether having scientific work written up in the lay press yielded extra citations. As it turns out, being published in the New York Times gave a boost of over 70% to the number of citations the papers would get.

### Slide 14

So, even though it’s by analogy, I think I can have some real confidence in saying what we’re doing here has some real value. But, I think if we take a more methodological approach to demonstrating why we believe what we believe, we can be even stronger.

Thank you for your time!

### Bonus slide

So, here’s a bad example I saw going across Facebook a month or two ago. A survey came out a bit ago saying that millennials are much more likely to want people to be in traditional gender roles in families (that dip right around 2014 on the left), which many media articles made great hay out of.

### Another bonus slide

A few links to look at.
But many folks found some real problems in the study--this dip reflects only 66 men, and if they had bothered to plot error bars they would be bigger than the observed dip. Some folks reran the numbers with larger bins (young Americans being 18-34 instead of 18-25) and the effect pretty much disappears.

So, hashtag-millennials, amirite?
